---
title: "A quick guide to nimbleNoBounds"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A quick guide to nimbleNoBounds}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



<!-- Add this vignette to the documentation via roxygen2::roxygenise() and  R CMD build -->

<!-- setwd("/home/pleydell/nimbleProject/nimble-snippets/nimbleAPT"); roxygen2::roxygenise() -->



This vignette assumes the reader is already familiar with using [nimble](https://r-nimble.org/) to perform Markov chain Monte Carlo (MCMC).

The principal motivation of this package is that bounds on prior distributions can impair MCMC performance when posterior distributions are located close to those bounds.
In particular, when using Metropolis Hastings, making proposals beyond known bounds, just to have them rejected, will increase rejection rates.
Moreover, in adaptive Metropolis Hastings, such increased rejection rates will lead to shrinking the Metropolis Hastings kernel, thereby increasing the auto-correlation in the accepted samples and reducing the effective sample size.

This package offers a simple solution to avoid these scenarios.
Common bounded probability distributions (beta, uniform, exponential, gamma) are transformed (via a change of variables) to the unbounded real line via
log or logit transformations.
The resulting distributions are provided for use in nimble models, which may increase MCMC efficiency.
We explore that claim here with a simple example.


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Toy example

The following toy problem compares sampling of bounded and unbounded distributions.

First, we create nimble models with both classical bounded distributions, and with unbounded transformed equivalents.

```{r model, fig.width=7, fig.height=7, message=FALSE, warning=FALSE}
library(nimbleNoBounds) # Also loads nimble
library(coda)           # For MCMC diagnostics


boundedDistributionsModel <- nimbleCode({
  b ~ dbeta(b1, b2)
  u ~ dunif(u1, u2)
  e ~ dexp(e1)
  g ~ dgamma(g1, g2)
})

unboundedDistributionsModel <- nimbleCode({
  timeB ~ dLogitBeta(b1, b2)
  timeU ~ dLogitUnif(u1, u2)
  te ~ dLogExp(e1)
  tg ~ dLogGamma(g1, g2)
  b <- ilogit(timeB)
  u <- (u2*exp(timeU)+u1)/(u2-u1)
  e <- exp(te)
  g <- exp(tg)
})

const = list(b1=1, b2=11, u1=-6, u2=66, e1=0.1, g1=0.1, g2=10)

rBounded   <- nimbleModel(boundedDistributionsModel, constants=const)
rUnbounded <- nimbleModel(unboundedDistributionsModel, constants=const)

cBounded   <- compileNimble(rBounded)
cUnbounded <- compileNimble(rUnbounded)

```

Now we perform MCMC using nimble's univariate Metropolis Hastings sampler.


```{r fitting, fig.width=7, fig.height=7, eval=FALSE, message=FALSE}
monitorNodes = c("b","u","e","g")

configureMcmcB = configureMCMC(cBounded,   monitors=monitorNodes)
configureMcmcU = configureMCMC(cUnbounded, monitors=monitorNodes)

configureMcmcB$removeSamplers()
configureMcmcB$addSampler("b")
configureMcmcB$addSampler("u")
configureMcmcB$addSampler("e")
configureMcmcB$addSampler("g")

configureMcmcU$removeSamplers()
configureMcmcU$addSampler("tb")
configureMcmcU$addSampler("tu")
configureMcmcU$addSampler("te")
configureMcmcU$addSampler("tg")

rMcmcBounded   <- buildMCMC(configureMcmcB)
rMcmcUnbounded <- buildMCMC(configureMcmcU)

cMcmcBounded   <- compileNimble(rMcmcBounded)
cMcmcUnbounded <- compileNimble(rMcmcUnbounded)

nReps = 10
gain = matrix(0, ncol=length(monitorNodes), nrow=nReps)
for (ii in 1:nReps) {
  if (ii %% 10 == 0)
    print(ii)
  ## Run MCMC
  timeB = system.time(cMcmcBounded$run(niter=1E6))["elapsed"]
  timeU = system.time(cMcmcUnbounded$run(niter=1E6))["elapsed"]
  ## Extract samples
  samplesB <- as.matrix(cMcmcBounded$mvSamples)
  samplesU <- as.matrix(cMcmcUnbounded$mvSamples)
  ## Calculate effective sample size
  effB = effectiveSize(samplesB)
  effU = effectiveSize(samplesU)
  ## Calculate sampling efficiency
  (efficiencyB = effB / timeB)
  (efficiencyU = effU / timeU)
  ## Calculate efficiency gain
  (efficiencyGain = efficiencyU / efficiencyB)
  gain[ii,] = efficiencyGain
}
colnames(gain) = c("Beta", "Unif", "Exp", "Gamma")  # monitorNodes

## write.csv(gain, file=here::here("data/gain.csv"), row.names = FALSE)

```
```{r gain, fig.width=7, fig.height=7, echo=FALSE, message=FALSE}
## load data gain, to avoid running slow loop above
data(gain)
```

Finally, some boxplots of the efficiency gain per distribution, with mean and 95% CIs.
```{r plotting, fig.width=7, fig.height=7, message=FALSE}
boxplot(log10(gain), ylab="log10 Efficiency Gain")
abline(h=0)
text(1, 1, paste("x", signif(mean(gain[,"Beta"]), 3)))
text(1, 0.8, paste0("(", paste(signif(quantile(gain[,"Beta"], c(0.025, 0.975)), 3), collapse=" - "), ")"))
text(2, 1, paste("x", signif(mean(gain[,"Unif"]), 3)))
text(2, 0.8, paste0("(", paste(signif(quantile(gain[,"Unif"], c(0.025, 0.975)), 3), collapse=" - "), ")"))
text(3, 1, paste("x", signif(mean(gain[,"Exp"]), 3)))
text(3, 0.8, paste0("(", paste(signif(quantile(gain[,"Exp"], c(0.025, 0.975)), 3), collapse=" - "), ")"))
text(4, 1, paste("x", signif(mean(gain[,"Gamma"]), 3)))
text(4, 0.8, paste0("(", paste(signif(quantile(gain[,"Gamma"], c(0.025, 0.975)), 3), collapse=" - "), ")"))


```

These results show that:
1. large efficiency gains can be obtained from sampling on the real line.
2. the size of the efficiency gain depends on the target distribution being sampled.
3. computational overhead arising from the transformations is negligable compared to potential efficiency gain from sampling on the unbounded real line.
