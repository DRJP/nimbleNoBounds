---
title: "A quick guide to nimbleNoBounds"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A quick guide to nimbleNoBounds}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



<!-- Add this vignette to the documentation via roxygen2::roxygenise() and  R CMD build -->

<!-- setwd("/home/pleydell/nimbleProject/nimble-snippets/nimbleAPT"); roxygen2::roxygenise() -->

# Introduction

This vignette assumes the reader is already familiar with using [nimble](https://r-nimble.org/) to perform Markov chain Monte Carlo (MCMC).

The principal motivation of this package is that bounds on prior distributions can impair MCMC performance when posterior distributions are located close to those bounds.
In particular, when using Metropolis Hastings, making proposals beyond known bounds, just to have them rejected, will increase rejection rates.
Moreover, in adaptive Metropolis Hastings, such increased rejection rates will lead to shrinking the Metropolis Hastings kernel, thereby increasing the auto-correlation in the accepted samples and reducing the effective sample size.

This package offers a simple solution to avoid these scenarios. Common bounded probability distributions (beta, unifom, exponential, gamma) are transformed to the unbounded real line via
log or logit transformations.


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Toy example

The following toy problem compares sampling of bounded and unbounded distributions.

First, we create nimble models with both classical bounded distributions, and with unbounded transformed equivalents.

```{r model, fig.width=7, fig.height=7}
library(nimbleNoBounds) # Also loads nimble

boundedCode <- nimbleCode({
  b ~ dbeta(b1, b2)
  u ~ dunif(u1, u2)
  e ~ dexp(e1)
  g ~ dgamma(g1, g2)
})

unboundedCode <- nimbleCode({
  timeB ~ dLogitBeta(b1, b2)
  timeU ~ dLogitUnif(u1, u2)
  te ~ dLogExp(e1)
  tg ~ dLogGamma(g1, g2)
  b <- ilogit(timeB)
  u <- (u2*exp(timeU)+u1)/(u2-u1)
  e <- exp(te)
  g <- exp(tg)
})

const = list(b1=1, b2=11, u1=-6, u2=66, e1=0.1, g1=0.1, g2=10)

rBounded   <- nimbleModel(boundedCode, constants=const)
rUnbounded <- nimbleModel(unboundedCode, constants=const)

cBounded   <- compileNimble(rBounded)
cUnbounded <- compileNimble(rUnbounded)


```

Now we perform MCMC using nimble's univariate Metropolis Hastings sampler.


```{r fitting1, fig.width=7, fig.height=7}
monitorNodes = c("b","u","e","g")

configureMcmcB = configureMCMC(cBounded,   monitors=monitorNodes)
configureMcmcU = configureMCMC(cUnbounded, monitors=monitorNodes)

configureMcmcB$removeSamplers()
configureMcmcB$addSampler("b")
configureMcmcB$addSampler("u")
configureMcmcB$addSampler("e")
configureMcmcB$addSampler("g")

configureMcmcU$removeSamplers()
configureMcmcU$addSampler("tb")
configureMcmcU$addSampler("tu")
configureMcmcU$addSampler("te")
configureMcmcU$addSampler("tg")

rMcmcBounded   <- buildMCMC(configureMcmcB)
rMcmcUnbounded <- buildMCMC(configureMcmcU)

cMcmcBounded   <- compileNimble(rMcmcBounded)
cMcmcUnbounded <- compileNimble(rMcmcUnbounded)

nReps = 10
gain = matrix(0, ncol=length(monitorNodes), nrow=nReps)
for (ii in 1:nReps) {
  if (ii %% 10 == 0)
    print(ii)
  ## Run MCMC
  timeB = system.time(cMcmcBounded$run(niter=1E6))["elapsed"]
  timeU = system.time(cMcmcUnbounded$run(niter=1E6))["elapsed"]
  ## Extract samples
  samplesB <- as.matrix(cMcmcBounded$mvSamples)
  samplesU <- as.matrix(cMcmcUnbounded$mvSamples)
  ## Calcualte effective sample size
  effB = coda::effectiveSize(samplesB)
  effU = coda::effectiveSize(samplesU)
  ## Calculate sampling efficiency
  (efficiencyB = effB / timeB)
  (efficiencyU = effU / timeU)
  ## Calcyulate efficiency gain
  (efficiencyGain = efficiencyU / efficiencyB)
  gain[ii,] = efficiencyGain
}
colnames(gain) = monitorNodes

boxplot

```

As we can see, the default MCMC scheme makes few, if any, jumps between the four potential modes.

So let's try with adaptive parallel tempering (APT).


```{r fitting2, fig.width=7, fig.height=7}

conf <- configureMCMC(cModel, nodes="centroids", monitors="centroids", enableWAIC = TRUE)
conf$removeSamplers()
conf$addSampler("centroids[1]", type="sampler_RW_tempered", control=list(temperPriors=TRUE))
conf$addSampler("centroids[2]", type="sampler_RW_tempered", control=list(temperPriors=TRUE))
conf

aptR <- buildAPT(conf, Temps=1:5, ULT= 1000, print=TRUE)

aptC <- compileNimble(aptR)

aptC$run(niter=15000)

samples <- tail(as.matrix(aptC$mvSamples), 10000)
summary(samples)

plot(samples, xlab="", ylab="", typ="l", xlim=c(-1,1)*max(cModel$y), ylim=c(-1,1)*max(cModel$y))
points(samples, col="red", pch=19, cex=0.1)
legend("topleft", legend=c("jumps", "samples"), col=c("black","red"), pch=c("_","X"), bg="white")

plot(as.mcmc(samples))

plot(as.mcmc(aptC$logProbs))

aptC$calculateWAIC()


```

We can see that jumps between nodes (black lines) are frequent and that each node has been sampled (red points).
The precision with which the weight of each posterior mode is estimated can be increased by increasing
1. the number of rungs in the temperature ladder, and moreover
2. the number of iterations (niter).

Finally, note that WAIC can be computed in exactly the same way as in nimble. See the section 'calculating WAIC' in the nimble manual for further details.
